{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from tensorflow.keras import  preprocessing, utils\n",
    "from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers.recurrent import LSTM,SimpleRNN\n",
    "import yaml\n",
    "import os\n",
    "import pandas as pd\n",
    "# from numba import jit, cuda\n",
    "import pickle\n",
    "from statistics import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", (tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_list =  os.listdir('dataset/Kaggledataset/')\n",
    "# files_list = ['dataset/Kaggledataset/food.yml', 'dataset/Kaggledataset/greetings.yml','dataset/Kaggledataset/humor.yml','dataset/Kaggledataset/movies.yml']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = list()\n",
    "answers = list()\n",
    "\n",
    "for filepath in files_list:\n",
    "    stream = open('dataset/Kaggledataset/' +filepath , 'rb')\n",
    "    docs = yaml.safe_load(stream)\n",
    "    conversations = docs['conversations']\n",
    "    for con in conversations:\n",
    "        if len( con ) > 2 :\n",
    "            questions.append(con[0])\n",
    "            replies = con[ 1 : ]\n",
    "            ans = ''\n",
    "            for rep in replies:\n",
    "                ans += ' ' + rep\n",
    "            answers.append( ans )\n",
    "        elif len( con )> 1:\n",
    "            questions.append(con[0])\n",
    "            answers.append(con[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# questions = list()\n",
    "# answers = list()\n",
    "# data_path = 'dataset/human_chat.txt'\n",
    "# with open(data_path, encoding='utf8') as file:\n",
    "#     for i , line in enumerate(file):\n",
    "#         try:\n",
    "#             if line[6] == '1':\n",
    "#                 questions.append(line[9:])\n",
    "#                 # questions1.append(line[8:])\n",
    "#             else:\n",
    "#                 answers.append(line[9:])\n",
    "#                 # answers1.append(line[8:])\n",
    "#         except:\n",
    "#             print(\"error at line :\" , i)\n",
    "        \n",
    "#         # print(i , len(questions) , len(answers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'dataset/dialogs.txt'\n",
    "# questions = list()\n",
    "# answers = list()\n",
    "with open(data_path, encoding='utf8') as file:\n",
    "    for i , line in enumerate(file):\n",
    "        q , a =line.split('\\t')\n",
    "        questions.append(q)\n",
    "        answers.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' Curious to dive deeper', ' Happy', ' Neutral', ' Surprised',\n",
       "       ' Disgusted', ' Sad', ' Fearful', ' Angry'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topicat_chat  = pd.read_csv('dataset/topical_chat.csv')\n",
    "topicat_chat.head()\n",
    "topicat_chat.sentiment.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question_ID</th>\n",
       "      <th>Questions</th>\n",
       "      <th>Answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1590140</td>\n",
       "      <td>What does it mean to have a mental illness?</td>\n",
       "      <td>Mental illnesses are health conditions that di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2110618</td>\n",
       "      <td>Who does mental illness affect?</td>\n",
       "      <td>It is estimated that mental illness affects 1 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6361820</td>\n",
       "      <td>What causes mental illness?</td>\n",
       "      <td>It is estimated that mental illness affects 1 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9434130</td>\n",
       "      <td>What are some of the warning signs of mental i...</td>\n",
       "      <td>Symptoms of mental health disorders vary depen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7657263</td>\n",
       "      <td>Can people with mental illness recover?</td>\n",
       "      <td>When healing from mental illness, early identi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Question_ID                                          Questions  \\\n",
       "0      1590140        What does it mean to have a mental illness?   \n",
       "1      2110618                    Who does mental illness affect?   \n",
       "2      6361820                        What causes mental illness?   \n",
       "3      9434130  What are some of the warning signs of mental i...   \n",
       "4      7657263            Can people with mental illness recover?   \n",
       "\n",
       "                                             Answers  \n",
       "0  Mental illnesses are health conditions that di...  \n",
       "1  It is estimated that mental illness affects 1 ...  \n",
       "2  It is estimated that mental illness affects 1 ...  \n",
       "3  Symptoms of mental health disorders vary depen...  \n",
       "4  When healing from mental illness, early identi...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mental_health = pd.read_csv('dataset/Mental_Health_FAQ.csv')\n",
    "mental_health.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(zip(mental_health.Questions, mental_health.Answers))\n",
    "for i in data:\n",
    "    questions.append(i[0])\n",
    "    answers.append(i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data\n",
    "del mental_health"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Show Number</th>\n",
       "      <th>Air Date</th>\n",
       "      <th>Round</th>\n",
       "      <th>Category</th>\n",
       "      <th>Value</th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>HISTORY</td>\n",
       "      <td>$200</td>\n",
       "      <td>For the last 8 years of his life, Galileo was ...</td>\n",
       "      <td>Copernicus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>ESPN's TOP 10 ALL-TIME ATHLETES</td>\n",
       "      <td>$200</td>\n",
       "      <td>No. 2: 1912 Olympian; football star at Carlisl...</td>\n",
       "      <td>Jim Thorpe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>EVERYBODY TALKS ABOUT IT...</td>\n",
       "      <td>$200</td>\n",
       "      <td>The city of Yuma in this state has a record av...</td>\n",
       "      <td>Arizona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>THE COMPANY LINE</td>\n",
       "      <td>$200</td>\n",
       "      <td>In 1963, live on \"The Art Linkletter Show\", th...</td>\n",
       "      <td>McDonald's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>EPITAPHS &amp; TRIBUTES</td>\n",
       "      <td>$200</td>\n",
       "      <td>Signer of the Dec. of Indep., framer of the Co...</td>\n",
       "      <td>John Adams</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Show Number    Air Date      Round                         Category  Value  \\\n",
       "0         4680  2004-12-31  Jeopardy!                          HISTORY   $200   \n",
       "1         4680  2004-12-31  Jeopardy!  ESPN's TOP 10 ALL-TIME ATHLETES   $200   \n",
       "2         4680  2004-12-31  Jeopardy!      EVERYBODY TALKS ABOUT IT...   $200   \n",
       "3         4680  2004-12-31  Jeopardy!                 THE COMPANY LINE   $200   \n",
       "4         4680  2004-12-31  Jeopardy!              EPITAPHS & TRIBUTES   $200   \n",
       "\n",
       "                                            Question      Answer  \n",
       "0  For the last 8 years of his life, Galileo was ...  Copernicus  \n",
       "1  No. 2: 1912 Olympian; football star at Carlisl...  Jim Thorpe  \n",
       "2  The city of Yuma in this state has a record av...     Arizona  \n",
       "3  In 1963, live on \"The Art Linkletter Show\", th...  McDonald's  \n",
       "4  Signer of the Dec. of Indep., framer of the Co...  John Adams  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jeopardy = pd.read_csv('dataset/JEOPARDY_CSV.csv')\n",
    "jeopardy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q , A = jeopardy[' Question'] , jeopardy[' Answer']\n",
    "questions.extend(Q)\n",
    "answers.extend(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "del Q\n",
    "del A\n",
    "del jeopardy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['sentimental', 'afraid', 'proud', 'faithful', 'terrified',\n",
       "       'joyful', 'angry', 'sad', 'jealous', 'grateful', 'prepared',\n",
       "       'embarrassed', 'excited', 'annoyed', 'lonely', 'ashamed', 'guilty',\n",
       "       'surprised', 'nostalgic', 'confident', 'furious', 'disappointed',\n",
       "       'caring', 'trusting', 'disgusted', 'anticipating', 'anxious',\n",
       "       'hopeful', 'content', 'impressed', 'apprehensive', 'devastated',\n",
       "       ' I really killed it!', ' we were in a different country',\n",
       "       't even like scary things',\n",
       "       't believe I like the show Power so much. I was never really into shows like that',\n",
       "       nan, ' time to jump on the motorcycle and go cruising!',\n",
       "       \" a boy.  I hear all these different labor stories that aren't exactly reassuring!  \",\n",
       "       't believe my daughter taught herself how to play the ukelele. I was amazed',\n",
       "       't think I wold like super heroes',\n",
       "       \"m so mad with my brother. He stole from me and didn't think I would notice. \",\n",
       "       \" but what I didn't know was that he was working in the next room with the door open.  He approached and asked what I had been saying.  I knew I was caught.  I was so disgusted with myself.  \",\n",
       "       '('], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_emotion = pd.read_csv('dataset/emotion-emotion_69k.csv')\n",
    "emotion_emotion.head()\n",
    "emotion_emotion.emotion.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(emotion_emotion['empathetic_dialogues'])):\n",
    "    qn = emotion_emotion['empathetic_dialogues'][i][10:-9]\n",
    "    if len(qn) != 0 and len(emotion_emotion['labels'][i]) !=0:\n",
    "        questions.append(qn)\n",
    "        answers.append(emotion_emotion['labels'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "285864 285864\n"
     ]
    }
   ],
   "source": [
    "print(len(questions) , len(answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "line  = list()\n",
    "emotion = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(emotion_emotion['Situation'])):\n",
    "    situation = emotion_emotion['Situation'][i]\n",
    "    if situation not in line:\n",
    "        line.append(str(situation))\n",
    "        emotion.append(str(emotion_emotion['emotion']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19205 19205\n"
     ]
    }
   ],
   "source": [
    "print(len(line) , len(emotion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(topicat_chat['message'])):     \n",
    "    line.append(topicat_chat['message'][i])\n",
    "    emotion.append(topicat_chat['sentiment'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207583 207583\n"
     ]
    }
   ],
   "source": [
    "print(len(line) , len(emotion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(line[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_emotion  = set()\n",
    "for i in emotion:\n",
    "    unique_emotion.add(i)\n",
    "len(unique_emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' Neutral', '0        sentimental\\n1        sentimental\\n2        sentimental\\n3        sentimental\\n4        sentimental\\n            ...     \\n64631    sentimental\\n64632    sentimental\\n64633      surprised\\n64634      surprised\\n64635      surprised\\nName: emotion, Length: 64636, dtype: object', ' Curious to dive deeper', ' Surprised', ' Happy', ' Sad', ' Angry', ' Disgusted', ' Fearful'}\n"
     ]
    }
   ],
   "source": [
    "print(unique_emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cleantext\n",
    "# @jit(target_backend='cuda')\n",
    "# def clean(x,y):\n",
    "#      Questions = list()\n",
    "#      Answers = list()\n",
    "#      QA = list()\n",
    "#      for i , sentences in enumerate(x):\n",
    "#           clean_question = (cleantext.clean(str(x[i]), extra_spaces=True, lowercase=True, stopwords=False, stemming=True, numbers=True, punct=True, clean_all = True))\n",
    "#           clean_answers = (cleantext.clean(str(y[i]), extra_spaces=True, lowercase=True, stopwords=False, stemming=True, numbers=True, punct=True, clean_all = True))\n",
    "#           qn = (( '<START> ' + clean_question + ' <END>' ).split()) \n",
    "#           an = (( '<START> ' + clean_answers + ' <END>' ).split()) \n",
    "#           Questions.append(qn)\n",
    "#           Answers.append(an)\n",
    "#           QA.append(qn)\n",
    "#           QA.append(an)\n",
    "#      return Questions , Answers ,QA\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Questions , Answers , QA = clean(questions,answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(Questions) , len(Answers) , len(QA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('QA.pickle','wb') as f:\n",
    "#     pickle.dump([Questions,Answers,QA],f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('QA.pickle','rb') as f:\n",
    "#     Questions,Answers,QA =pickle.load(f)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec = Word2Vec(QA , min_count=0 , window=5,workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(word2vec.wv['what'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @jit(target_backend='cuda')  \n",
    "# def replacer(model,A):\n",
    "#     X = list()\n",
    "#     for  i , sentences in enumerate(A):\n",
    "#         # print(type(len(sentences)))\n",
    "#         q = np.zeros((len(sentences),100),float)\n",
    "#         # try:\n",
    "#         for j ,words in enumerate(sentences):\n",
    "#                 q[j] =(model.wv[words].flatten())\n",
    "#         X.append(q)\n",
    "#         # except:\n",
    "#         #     print(\"Error in  line no :\" , i ,\"\\n\")\n",
    "#     return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_input = replacer(word2vec,Questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_output = replacer(word2vec,Answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_input = np.array(train_input)\n",
    "# train_output = np.array(train_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_with_tags = list()\n",
    "for i in range( len( answers ) ):\n",
    "    if type( answers[i] ) == str:\n",
    "        answers_with_tags.append( answers[i] )\n",
    "    else:\n",
    "        questions.pop( i )\n",
    "\n",
    "answers = list()\n",
    "for i in range( len( answers_with_tags ) ) :\n",
    "    answers.append( '<START> ' + answers_with_tags[i] + ' <END>' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB SIZE : 108717\n"
     ]
    }
   ],
   "source": [
    "# import re\n",
    "tokenizer = preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts( questions + answers )\n",
    "VOCAB_SIZE = len( tokenizer.word_index )+1\n",
    "print( 'VOCAB SIZE : {}'.format( VOCAB_SIZE ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QqYoDsbSCo4f"
   },
   "outputs": [],
   "source": [
    "vocab = []\n",
    "for word in tokenizer.word_index:\n",
    "  vocab.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen_questions = 10\n",
    "maxlen_answers = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(285860, 10) 10\n"
     ]
    }
   ],
   "source": [
    "tokenized_questions = tokenizer.texts_to_sequences( questions  )\n",
    "# maxlen_questions = mode( [len(i) for i in tokenized_questions ] )\n",
    "# print(maxlen_questions)\n",
    "padded_questions = preprocessing.sequence.pad_sequences( tokenized_questions, maxlen = maxlen_questions, padding = 'post' ,truncating= 'post')\n",
    "encoder_input_data = np.array(padded_questions)\n",
    "print(encoder_input_data.shape, maxlen_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(285860, 10) 10\n"
     ]
    }
   ],
   "source": [
    "tokenized_answers = tokenizer.texts_to_sequences( answers  )\n",
    "# maxlen_answers = mode( [len(i) for i in tokenized_answers ] )\n",
    "\n",
    "padded_answers = np.array(preprocessing.sequence.pad_sequences( tokenized_answers , maxlen= maxlen_answers , padding='post',truncating= 'post' ))\n",
    "decoder_input_data = np.array( padded_answers )\n",
    "print( decoder_input_data.shape , maxlen_answers )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(285860, 10)\n"
     ]
    }
   ],
   "source": [
    "tokenized_answers = tokenizer.texts_to_sequences( answers )\n",
    "for i in range(len(tokenized_answers)) :\n",
    "    tokenized_answers[i] = tokenized_answers[i][1:]\n",
    "padded_answers1 = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post',truncating= 'post'  )\n",
    "# onehot_answers = utils.to_categorical( padded_answers1 , num_classes= VOCAB_SIZE ,dtype = 'float32' )\n",
    "decoder_output_data = np.array( padded_answers1 )\n",
    "print( decoder_output_data.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_answers = tokenizer.texts_to_sequences( answers[:5000]   )\n",
    "# tokenized_questions = tokenizer.texts_to_sequences( questions[:5000]   )\n",
    "# maxlen_questions = mode( [len(i) for i in tokenized_questions ] )\n",
    "# maxlen_answers = mode( [len(i) for i in tokenized_answers ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def my_numpy_func(y_true, y_pred):\n",
    "#   # y_true and y_pred are already numpy arrays\n",
    "#   # put all numpy or sklearn codes here\n",
    "#   y_true = utils.to_categorical( y_true , num_classes= VOCAB_SIZE ,dtype = 'float32' )\n",
    "#   cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "#   return cce(y_true, y_pred)\n",
    "\n",
    "# @tf.function\n",
    "# def custom_metric(y_true, y_pred):\n",
    "#   # y_true and y_pred are tensors\n",
    "#   # no numpy or sklearn code is allowed here\n",
    "#   score = tf.numpy_function(my_numpy_func, [y_true, y_pred], tf.float32)\n",
    "#   return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss_function(y_true, y_pred):\n",
    "   y1 = tf.numpy_function(utils.to_categorical , [y_true ,VOCAB_SIZE] , tf.float32) \n",
    "   cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "   # y2 = utils.to_categorical(y_true , VOCAB_SIZE )\n",
    "   # print(y1[0] , y2[0])\n",
    "   loss = cce(y1, y_pred)\n",
    "   # del y1\n",
    "   return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 440
    },
    "colab_type": "code",
    "id": "W3YjCFDwPRVN",
    "outputId": "7bc112a0-6945-4100-e8d9-3bc5691797e5"
   },
   "outputs": [],
   "source": [
    "# encoder_inputs = tf.keras.layers.Input(shape=( maxlen_questions , ))\n",
    "# encoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, 200 , mask_zero=True ) (encoder_inputs)\n",
    "# encoder_outputs , state_h , state_c = tf.keras.layers.LSTM( 200 , return_state=True )( encoder_embedding )\n",
    "# encoder_states = [ state_h , state_c ]\n",
    "\n",
    "# decoder_inputs = tf.keras.layers.Input(shape=( maxlen_answers ,  ))\n",
    "# decoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, 200 , mask_zero=True) (decoder_inputs)\n",
    "# decoder_lstm = tf.keras.layers.LSTM( 200 , return_state=True , return_sequences=True )\n",
    "# decoder_outputs , _ , _ = decoder_lstm ( decoder_embedding , initial_state=encoder_states )\n",
    "# decoder_dense = tf.keras.layers.Dense( VOCAB_SIZE , activation=tf.keras.activations.softmax ) \n",
    "# output = decoder_dense ( decoder_outputs )\n",
    "\n",
    "# model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = tf.keras.layers.Input(shape=( maxlen_questions , ))\n",
    "encoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, 100 , mask_zero=True ) (encoder_inputs)\n",
    "# encoder_outputs , state_h , state_c = tf.keras.layers.LSTM( 100 , return_state=True )( encoder_embedding )\n",
    "# encoder_states = [ state_h , state_c ]\n",
    "\n",
    "decoder_inputs = tf.keras.layers.Input(shape=( maxlen_answers ,  ))\n",
    "decoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, 100 , mask_zero=True) (decoder_inputs)\n",
    "\n",
    "decoder_lstm = tf.keras.layers.LSTM( 200 , return_state=True , return_sequences=True )\n",
    "concatenate = tf.keras.layers.Concatenate( name='Concatenate_1')([encoder_embedding,decoder_embedding])\n",
    "decoder_outputs , _ , _ = decoder_lstm ( concatenate )\n",
    "\n",
    "decoder_dense = tf.keras.layers.Dense( VOCAB_SIZE , activation=tf.keras.activations.softmax ) \n",
    "output = decoder_dense ( decoder_outputs )\n",
    "\n",
    "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = tf.keras.layers.Input(shape=( maxlen_questions , ))\n",
    "encoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, 100 , mask_zero=True ) (encoder_inputs)\n",
    "encoder_outputs , state_h , state_c = tf.keras.layers.LSTM( 100 , return_state=True )( encoder_embedding )\n",
    "encoder_states = [ state_h , state_c ]\n",
    "\n",
    "decoder_inputs = tf.keras.layers.Input(shape=( maxlen_answers ,  ))\n",
    "decoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, 100 , mask_zero=True) (decoder_inputs)\n",
    "decoder_lstm = tf.keras.layers.LSTM( 100 , return_state=True , return_sequences=True )\n",
    "decoder_outputs , _ , _ = decoder_lstm ( decoder_embedding , initial_state=encoder_states )\n",
    "decoder_lstm1 = tf.keras.layers.LSTM( 100 , return_state=True , return_sequences=True )\n",
    "decoder_outputs1 , _ , _ = decoder_lstm1 ( decoder_outputs )\n",
    "decoder_dense = tf.keras.layers.Dense( VOCAB_SIZE , activation=tf.keras.activations.softmax ) \n",
    "output = decoder_dense ( decoder_outputs1 )\n",
    "\n",
    "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_13 (InputLayer)           [(None, 10)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_14 (InputLayer)           [(None, 10)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_12 (Embedding)        (None, 10, 100)      10871700    input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_13 (Embedding)        (None, 10, 100)      10871700    input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "Concatenate_1 (Concatenate)     (None, 10, 200)      0           embedding_12[0][0]               \n",
      "                                                                 embedding_13[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lstm_11 (LSTM)                  [(None, 10, 200), (N 320800      Concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 10, 108717)   21852117    lstm_11[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 43,916,317\n",
      "Trainable params: 43,916,317\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss=custom_loss_function , run_eagerly= False )\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    }
   ],
   "source": [
    "model.fit([encoder_input_data, decoder_input_data], decoder_output_data, batch_size= 50, epochs= 3 ) \n",
    "model.save( 'mo.h5' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DataGenerator(tf.keras.utils.Sequence):\n",
    "#   def __init__(self, x, x1, y, batch_size = 38, shuffle=True):\n",
    "#     super().__init__()\n",
    "#     self.x = x\n",
    "#     self.x1 = x1\n",
    "#     self.y = y\n",
    "#     self.batch_size = batch_size\n",
    "#     self.shuffle = shuffle\n",
    "#     key_array = []\n",
    "#     self.key_array = np.arange(self.x.shape[0])\n",
    "#     self.on_epoch_end()\n",
    "\n",
    "#   def __len__(self):\n",
    "#     return len(self.key_array)//self.batch_size\n",
    "\n",
    "#   def __getitem__(self, index):\n",
    "#     keys = self.key_array[index*self.batch_size:(index+1)*self.batch_size]\n",
    "#     x = np.asarray(self.x[keys], dtype=np.float32)\n",
    "#     x1 = np.asarray(self.x1[keys], dtype=np.float32)\n",
    "#     y = np.asarray(self.y[keys], dtype=np.float32)\n",
    "#     return x,x1, y\n",
    "\n",
    "#   def on_epoch_end(self):\n",
    "#     if self.shuffle:\n",
    "#       self.key_array = np.random.permutation(self.key_array)\n",
    "      \n",
    "\n",
    "# generator = DataGenerator(x=encoder_input_data, x1 =decoder_input_data , y = decoder_output_data, batch_size= 50, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(generator)\n",
    "# print( encoder_input_data.shape )\n",
    "# print( decoder_input_data.shape )\n",
    "# print( decoder_output_data.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(generator)/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_epochs = 10\n",
    "# n_batches = len(generator)\n",
    "# loss_train = np.zeros(shape=(n_epochs,), dtype=np.float32)\n",
    "# acc_train = np.zeros(shape=(n_epochs,), dtype=np.float32)\n",
    "# loss_val = np.zeros(shape=(n_epochs,))\n",
    "# acc_val = np.zeros(shape=(n_epochs,))\n",
    "# opt = tf.keras.optimizers.RMSprop()\n",
    "# cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "# for epoch in range(n_epochs):\n",
    "#   print(\"\\nEpoch = %d\" % (epoch,))\n",
    "#   epoch_loss_avg = tf.keras.metrics.Mean() # Keeping track of the training loss\n",
    "#   epoch_acc_avg = tf.keras.metrics.Mean() # Keeping track of the training accuracy\n",
    "#   start = time.time()\n",
    "#   for batch in range(n_batches):\n",
    "#     x1 ,x2, y = generator[batch]\n",
    "#     # print(len(y))\n",
    "#     y1 = utils.to_categorical( y , num_classes= VOCAB_SIZE ,dtype = 'float32' )\n",
    "\n",
    "#     with tf.GradientTape() as tape: # Forward pass\n",
    "#       y_ = model((x1 , x2), training=True)\n",
    "#       loss = cce(y1 , y_)\n",
    "      \n",
    "#     grad = tape.gradient(loss, model.trainable_variables) # Backpropagation\n",
    "#     # print(grad)\n",
    "#     opt.apply_gradients(zip(grad, model.trainable_variables)) # Update network weights\n",
    "    \n",
    "#     epoch_loss_avg(loss)\n",
    "#     # epoch_acc_avg(accuracy_score(y_true=y, y_pred=np.argmax(y_, axis=-1)))\n",
    "#     if batch % 600 == 0:\n",
    "#             print(\n",
    "#                 \"Epoch %d : Batch %d : Loss %.4f\"\n",
    "#                 % (epoch , batch, float(loss))\n",
    "#             )\n",
    "\n",
    "  \n",
    "#   end = time.time()\n",
    "#   loss_train[epoch] = epoch_loss_avg.result()\n",
    "#   acc_train[epoch] = epoch_acc_avg.result()\n",
    "\n",
    "#   print('---- Training ----')\n",
    "#   print('Epoch =  {0:.3f}'.format(epoch))\n",
    "#   print('Loss  =  {0:.3f}'.format(loss_train[epoch]))\n",
    "#   print('Acc   =  {0:.3f}'.format(acc_train[epoch]))\n",
    "#   print('Time  =  {0:.3f}'.format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss_function(y_true, y_pred):\n",
    "   cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "   y_true = utils.to_categorical( y_true , num_classes= VOCAB_SIZE ,dtype = 'float32' )\n",
    "   loss = cce(y_true, y_pred).numpy()\n",
    "   return tf.add(loss , 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('custom1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_inference_models():\n",
    "    \n",
    "    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
    "    \n",
    "    decoder_state_input_h = tf.keras.layers.Input(shape=( 200 ,))\n",
    "    decoder_state_input_c = tf.keras.layers.Input(shape=( 200 ,))\n",
    "    \n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    \n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "        decoder_embedding , initial_state=decoder_states_inputs)\n",
    "    \n",
    "    decoder_states = [state_h, state_c]\n",
    "\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    \n",
    "    decoder_model = tf.keras.models.Model(\n",
    "        [decoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs] + decoder_states)\n",
    "    \n",
    "    return encoder_model , decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_tokens( sentence : str ):\n",
    "\n",
    "    words = sentence.lower().split()\n",
    "    tokens_list = list()\n",
    "  \n",
    "    for word in words:\n",
    "        tokens_list.append( tokenizer.word_index[ word ] ) \n",
    "    return preprocessing.sequence.pad_sequences( [tokens_list] , maxlen=maxlen_questions , padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_model , dec_model = make_inference_models()\n",
    "\n",
    "for _ in range(10):\n",
    "    states_values = enc_model.predict( str_to_tokens( input( 'Enter question : ' ) ) )\n",
    "    empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
    "    empty_target_seq[0, 0] = tokenizer.word_index['start']\n",
    "    stop_condition = False\n",
    "    decoded_translation = ''\n",
    "    while not stop_condition :\n",
    "        dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )\n",
    "        sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n",
    "        sampled_word = None\n",
    "        for word , index in tokenizer.word_index.items() :\n",
    "            if sampled_word_index == index :\n",
    "                decoded_translation += ' {}'.format( word )\n",
    "                sampled_word = word\n",
    "        \n",
    "        if sampled_word == 'end' or len(decoded_translation.split()) > maxlen_answers:\n",
    "            stop_condition = True\n",
    "            \n",
    "        empty_target_seq = np.zeros( ( 1 , 1 ) )  \n",
    "        empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
    "        states_values = [ h , c ] \n",
    "\n",
    "    print( decoded_translation )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('tfgpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "04ee3065d2333376d0aa8ad1893b6f4d1e1600d75ce606d1ee7a0507ce07e97f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
