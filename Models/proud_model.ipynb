{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from tensorflow.keras import  preprocessing, utils\n",
    "from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers.recurrent import LSTM,SimpleRNN\n",
    "import yaml\n",
    "import os\n",
    "import pandas as pd\n",
    "# from numba import jit, cuda\n",
    "import pickle\n",
    "from statistics import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = preprocessing.text.Tokenizer(oov_token =1)\n",
    "def token(questions , answers):\n",
    "    # tokenizer = preprocessing.text.Tokenizer()\n",
    "    tokenizer.fit_on_texts( questions + answers )\n",
    "    VOCAB_SIZE = len( tokenizer.word_index )+1\n",
    "    # print( 'VOCAB SIZE : {}'.format( VOCAB_SIZE ))\n",
    "\n",
    "\n",
    "    vocab = []\n",
    "    for word in tokenizer.word_index:\n",
    "        vocab.append(word)\n",
    "    \n",
    "    maxlen_questions = int(np.ceil(np.mean([len(q) for q in questions])))\n",
    "    maxlen_answers = int(np.ceil(np.mean([len(a) for a in answers])))\n",
    "    \n",
    "    print(maxlen_questions , maxlen_answers)\n",
    "    tokenized_questions = tokenizer.texts_to_sequences( questions  )\n",
    "    padded_questions = preprocessing.sequence.pad_sequences( tokenized_questions, maxlen = maxlen_questions, padding = 'post' ,truncating= 'post')\n",
    "    encoder_input_data = np.array(padded_questions)\n",
    "\n",
    "\n",
    "    tokenized_answers = tokenizer.texts_to_sequences( answers  )\n",
    "    padded_answers = np.array(preprocessing.sequence.pad_sequences( tokenized_answers , maxlen= maxlen_answers , padding='post',truncating= 'post' ))\n",
    "    decoder_input_data = np.array( padded_answers )\n",
    "\n",
    "\n",
    "    for i in range(len(tokenized_answers)) :\n",
    "        tokenized_answers[i] = tokenized_answers[i][1:]\n",
    "    padded_answers1 = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post',truncating= 'post'  )\n",
    "    # onehot_answers = utils.to_categorical( padded_answers1 , num_classes= VOCAB_SIZE ,dtype = 'float32' )\n",
    "    decoder_output_data = np.array( padded_answers1 )\n",
    "    return VOCAB_SIZE ,maxlen_questions, maxlen_answers,encoder_input_data ,decoder_input_data ,decoder_output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('../processed_dataset/proud.pickle','rb+') as f: \n",
    "  questions , answers =  pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 73\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE, maxlen_questions, maxlen_answers, encoder_input_data, decoder_input_data, decoder_output_data = token(questions , answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5007 65 73 (4251, 65) (4251, 73) (4251, 73)\n"
     ]
    }
   ],
   "source": [
    "print(VOCAB_SIZE, maxlen_questions, maxlen_answers, encoder_input_data.shape, decoder_input_data.shape, decoder_output_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss_function(y_true, y_pred):\n",
    "   y1 = tf.numpy_function(utils.to_categorical , [y_true ,VOCAB_SIZE] , tf.float32) \n",
    "   cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "   # y2 = utils.to_categorical(y_true , VOCAB_SIZE )\n",
    "   # print(y1[0] , y2[0])\n",
    "   loss = cce(y1, y_pred)\n",
    "   # del y1\n",
    "   return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 65)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 73)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 65, 200)      1001400     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 73, 200)      1001400     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 200), (None, 320800      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 73, 200), (N 320800      embedding_1[0][0]                \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 73, 5007)     1006407     lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 3,650,807\n",
      "Trainable params: 3,650,807\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# encoder_inputs = tf.keras.layers.Input(shape=( maxlen_questions , ))\n",
    "# encoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE ,200, mask_zero=True ) ( encoder_inputs )\n",
    "# encoder_lstm1 = tf.keras.layers.LSTM ( 200 , return_state=True , return_sequences=True)\n",
    "# encoder_lstm2 = tf.keras.layers.LSTM ( 200 , return_state=True , return_sequences=True)\n",
    "# encoder_output1 , state_h1 , state_c1 = encoder_lstm1(encoder_embedding )\n",
    "# encoded_state1 = [state_h1 , state_c1]\n",
    "# encoder_output2 , state_h2 , state_c2 = encoder_lstm2(encoder_output1)\n",
    "# encoded_state2 = [state_h2 , state_c2]\n",
    "\n",
    "# decoder_inputs = tf.keras.layers.Input(shape=( maxlen_answers ,  ))\n",
    "# decoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE ,200, mask_zero=True ) ( decoder_inputs )\n",
    "# decoder_lstm1 = tf.keras.layers.LSTM( 200 , return_state=True , return_sequences=True)\n",
    "# decoder_lstm2 = tf.keras.layers.LSTM( 200 , return_state=True , return_sequences=True)\n",
    "# decoder_output1 ,state_h3 , state_c3 = decoder_lstm1 (decoder_embedding ,  initial_state=encoded_state1)\n",
    "# decoder_state21 = [state_h3 , state_c3]\n",
    "# decoder_lstm_output1 = [decoder_output1]\n",
    "# decoder_output2 ,state_h4 ,state_c4  = decoder_lstm2 (decoder_output1 ,  initial_state=encoded_state2)\n",
    "# decoder_state2 = [state_h4 , state_c4]\n",
    "# decoder_dense = tf.keras.layers.Dense( VOCAB_SIZE , activation=tf.keras.activations.softmax)\n",
    "# output = decoder_dense (decoder_output2)\n",
    "\n",
    "encoder_inputs = tf.keras.layers.Input(shape=( maxlen_questions , ))\n",
    "encoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, 200 , mask_zero=True ) (encoder_inputs)\n",
    "encoder_outputs , state_h , state_c = tf.keras.layers.LSTM( 200 , return_state=True )( encoder_embedding )\n",
    "encoder_states = [ state_h , state_c ]\n",
    "\n",
    "decoder_inputs = tf.keras.layers.Input(shape=( maxlen_answers ,  ))\n",
    "decoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, 200 , mask_zero=True) (decoder_inputs)\n",
    "decoder_lstm = tf.keras.layers.LSTM( 200 , return_state=True , return_sequences=True )\n",
    "decoder_outputs , _ , _ = decoder_lstm ( decoder_embedding , initial_state=encoder_states )\n",
    "decoder_dense = tf.keras.layers.Dense( VOCAB_SIZE , activation=tf.keras.activations.softmax ) \n",
    "output = decoder_dense ( decoder_outputs )\n",
    "model3 = tf.keras.models.Model([encoder_inputs, decoder_inputs], output )\n",
    "model3.compile(optimizer=tf.keras.optimizers.RMSprop(), loss=custom_loss_function , run_eagerly= False )\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "86/86 [==============================] - 107s 1s/step - loss: 0.3348\n",
      "Epoch 2/15\n",
      "86/86 [==============================] - 91s 1s/step - loss: 0.1895\n",
      "Epoch 3/15\n",
      "86/86 [==============================] - 92s 1s/step - loss: 0.1822\n",
      "Epoch 4/15\n",
      "86/86 [==============================] - 93s 1s/step - loss: 0.1770\n",
      "Epoch 5/15\n",
      "86/86 [==============================] - 94s 1s/step - loss: 0.1721\n",
      "Epoch 6/15\n",
      "86/86 [==============================] - 93s 1s/step - loss: 0.1679\n",
      "Epoch 7/15\n",
      "86/86 [==============================] - 95s 1s/step - loss: 0.1625\n",
      "Epoch 8/15\n",
      "86/86 [==============================] - 94s 1s/step - loss: 0.1579\n",
      "Epoch 9/15\n",
      "86/86 [==============================] - 95s 1s/step - loss: 0.1545\n",
      "Epoch 10/15\n",
      "86/86 [==============================] - 93s 1s/step - loss: 0.1518\n",
      "Epoch 11/15\n",
      "86/86 [==============================] - 95s 1s/step - loss: 0.1486\n",
      "Epoch 12/15\n",
      "86/86 [==============================] - 94s 1s/step - loss: 0.1456\n",
      "Epoch 13/15\n",
      "86/86 [==============================] - 94s 1s/step - loss: 0.1431\n",
      "Epoch 14/15\n",
      "86/86 [==============================] - 93s 1s/step - loss: 0.1407\n",
      "Epoch 15/15\n",
      "86/86 [==============================] - 92s 1s/step - loss: 0.1385\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    model3.fit([encoder_input_data, decoder_input_data], decoder_output_data, batch_size= 50, epochs= 15 ) \n",
    "    model3.save( 'proud.h5' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 80) for input KerasTensor(type_spec=TensorSpec(shape=(None, 80), dtype=tf.float32, name='input_24'), name='input_24', description=\"created by layer 'input_24'\"), but it was called on an input with incompatible shape (None, 1).\n",
      " i am sorry to hear that end\n",
      " i hope you feel better end\n",
      " i am so sorry to hear that end\n",
      " i am sorry to hear that end\n",
      " oh no did you have a good time end\n",
      " oh no did you have a good time end\n",
      " oh no i am sorry to hear that end\n",
      " i am sorry to hear that end\n",
      " i hope you feel better end\n",
      " i hope you feel better end\n"
     ]
    }
   ],
   "source": [
    "def make_inference_models():\n",
    "    \n",
    "    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
    "    \n",
    "    decoder_state_input_h = tf.keras.layers.Input(shape=( 200 ,))\n",
    "    decoder_state_input_c = tf.keras.layers.Input(shape=( 200 ,))\n",
    "    \n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    \n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "        decoder_embedding , initial_state=decoder_states_inputs)\n",
    "    \n",
    "    decoder_states = [state_h, state_c]\n",
    "\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    \n",
    "    decoder_model = tf.keras.models.Model(\n",
    "        [decoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs] + decoder_states)\n",
    "    \n",
    "    return encoder_model , decoder_model\n",
    "def str_to_tokens( sentence : str ):\n",
    "\n",
    "    words = sentence.lower().split()\n",
    "    tokens_list = list()\n",
    "  \n",
    "    for word in words:\n",
    "        tokens_list.append( tokenizer.word_index[ word ] ) \n",
    "    return preprocessing.sequence.pad_sequences( [tokens_list] , maxlen=maxlen_questions , padding='post')\n",
    "enc_model , dec_model = make_inference_models()\n",
    "\n",
    "for _ in range(10):\n",
    "    states_values = enc_model.predict( str_to_tokens( input( 'Enter question : ' ) ) )\n",
    "    empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
    "    empty_target_seq[0, 0] = tokenizer.word_index['start']\n",
    "    stop_condition = False\n",
    "    decoded_translation = ''\n",
    "    while not stop_condition :\n",
    "        dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )\n",
    "        sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n",
    "        sampled_word = None\n",
    "        for word , index in tokenizer.word_index.items() :\n",
    "            if sampled_word_index == index :\n",
    "                decoded_translation += ' {}'.format( word )\n",
    "                sampled_word = word\n",
    "        \n",
    "        if sampled_word == 'end' or len(decoded_translation.split()) > maxlen_answers:\n",
    "            stop_condition = True\n",
    "            \n",
    "        empty_target_seq = np.zeros( ( 1 , 1 ) )  \n",
    "        empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
    "        states_values = [ h , c ] \n",
    "\n",
    "    print( decoded_translation )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_tokens( sentence : str ):\n",
    "\n",
    "    words = sentence.lower().split()\n",
    "    tokens_list = list()\n",
    "  \n",
    "    for word in words:\n",
    "        tokens_list.append( tokenizer.word_index[ word ] ) \n",
    "    return preprocessing.sequence.pad_sequences( [tokens_list] , maxlen=maxlen_questions , padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_model():\n",
    "    \n",
    "    encoder_output1 , state_h1, state_c1 = encoder_lstm(encoder_embedding)\n",
    "    encoded_state1 = [state_h1, state_c1]\n",
    "\n",
    "    encoder_output2 , state_h2, state_c2 = encoder_lstm(encoder_output1)\n",
    "    encoded_state2 = [state_h2, state_c2]\n",
    "\n",
    "    encoder_model = tf.keras.models.Model (encoder_inputs, [encoded_state1] + [encoded_state2])\n",
    "    \n",
    "    return encoder_model\n",
    "    # encoder_output1 , state_h1, state_c1 = encoder_lstm1(encoder_embedding)\n",
    "    # encoded_state1 = [state_h1, state_c1]\n",
    "\n",
    "    # encoder_output2 , state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
    "    # encoded_state2 = [state_h2, state_c2]\n",
    "\n",
    "    # encoder_model = tf.keras.models.Model (encoder_inputs, [encoded_state2] + [encoded_state1])\n",
    "    \n",
    "    return encoder_model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_model():    \n",
    "    decoder_state_input_h1 = tf.keras.layers.Input(shape=( 200 ,))\n",
    "    decoder_state_input_c1 = tf.keras.layers.Input(shape=( 200 ,))\n",
    "\n",
    "    decoder_state_input_h2 = tf.keras.layers.Input(shape=( 200 ,))\n",
    "    decoder_state_input_c2 = tf.keras.layers.Input(shape=( 200 ,))\n",
    "    \n",
    "    decoder_states_input1 = [decoder_state_input_h1, decoder_state_input_c1]\n",
    "    decoder_states_input2 = [decoder_state_input_h2, decoder_state_input_c2]\n",
    "\n",
    "    decoder_output1, state_h1, state_c1 = decoder_lstm(\n",
    "        decoder_embedding , initial_state=decoder_states_input1)\n",
    "\n",
    "    decoder_output2, state_h2, state_c2 = decoder_lstm(\n",
    "        decoder_output1 , initial_state= decoder_states_input2)\n",
    "\n",
    "    decoder_outputs = decoder_dense(decoder_output2)\n",
    "    decoder_states = [state_h2 ,state_c2]\n",
    "   \n",
    "    decoder_model = tf.keras.models.Model(\n",
    "        [decoder_inputs] + decoder_states_input1 + decoder_states_input2,\n",
    "        [decoder_outputs] + decoder_states)\n",
    "    \n",
    "    # decoder_state_input_h1 = tf.keras.layers.Input(shape=( 200 ,))\n",
    "    # decoder_state_input_c1 = tf.keras.layers.Input(shape=( 200 ,))\n",
    "\n",
    "    # decoder_state_input_h2 = tf.keras.layers.Input(shape=( 200 ,))\n",
    "    # decoder_state_input_c2 = tf.keras.layers.Input(shape=( 200 ,))\n",
    "    \n",
    "    # decoder_states_input1 = [decoder_state_input_h1, decoder_state_input_c1]\n",
    "    # decoder_states_input2 = [decoder_state_input_h2, decoder_state_input_c2]\n",
    "\n",
    "    # decoder_output1, state_h1, state_c1 = decoder_lstm1(\n",
    "    #     decoder_embedding , initial_state=decoder_states_input1)\n",
    "\n",
    "    # decoder_output2, state_h2, state_c2 = decoder_lstm2(\n",
    "    #     decoder_output1 , initial_state= decoder_states_input2)\n",
    "\n",
    "    # decoder_outputs = decoder_dense(decoder_output2)\n",
    "    # decoder_states = [state_h2 ,state_c2]\n",
    "   \n",
    "    # decoder_model = tf.keras.models.Model(\n",
    "    #     [decoder_inputs] + decoder_states_input1 + decoder_states_input2,\n",
    "    #     [decoder_outputs] + decoder_states)\n",
    "    return decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 80) for input KerasTensor(type_spec=TensorSpec(shape=(None, 80), dtype=tf.float32, name='input_6'), name='input_6', description=\"created by layer 'input_6'\"), but it was called on an input with incompatible shape (None, 1).\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn [35], line 40\u001b[0m\n",
      "\u001b[0;32m     37\u001b[0m decoded_translation \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n",
      "\u001b[0;32m     38\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m stop_condition :\n",
      "\u001b[0;32m     39\u001b[0m     \u001b[39m# print(\"alpha\")\u001b[39;00m\n",
      "\u001b[1;32m---> 40\u001b[0m     dec_outputs , h , c \u001b[39m=\u001b[39m dec_model\u001b[39m.\u001b[39;49mpredict([ empty_target_seq ] \u001b[39m+\u001b[39;49m states_values1 \u001b[39m+\u001b[39;49m states_values2 )\n",
      "\u001b[0;32m     41\u001b[0m     \u001b[39m# print(\"beta\")\u001b[39;00m\n",
      "\u001b[0;32m     42\u001b[0m     sampled_word_index \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax( dec_outputs[\u001b[39m0\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :] )\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\keras\\engine\\training.py:1720\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n",
      "\u001b[0;32m   1714\u001b[0m   \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n",
      "\u001b[0;32m   1715\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\u001b[39m'\u001b[39m\u001b[39mUsing Model.predict with \u001b[39m\u001b[39m'\u001b[39m\n",
      "\u001b[0;32m   1716\u001b[0m                   \u001b[39m'\u001b[39m\u001b[39mMultiWorkerDistributionStrategy or TPUStrategy and \u001b[39m\u001b[39m'\u001b[39m\n",
      "\u001b[0;32m   1717\u001b[0m                   \u001b[39m'\u001b[39m\u001b[39mAutoShardPolicy.FILE might lead to out-of-order result\u001b[39m\u001b[39m'\u001b[39m\n",
      "\u001b[0;32m   1718\u001b[0m                   \u001b[39m'\u001b[39m\u001b[39m. Consider setting it to AutoShardPolicy.DATA.\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32m-> 1720\u001b[0m data_handler \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39;49mget_data_handler(\n",
      "\u001b[0;32m   1721\u001b[0m     x\u001b[39m=\u001b[39;49mx,\n",
      "\u001b[0;32m   1722\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n",
      "\u001b[0;32m   1723\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49msteps,\n",
      "\u001b[0;32m   1724\u001b[0m     initial_epoch\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n",
      "\u001b[0;32m   1725\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n",
      "\u001b[0;32m   1726\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n",
      "\u001b[0;32m   1727\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n",
      "\u001b[0;32m   1728\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n",
      "\u001b[0;32m   1729\u001b[0m     model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n",
      "\u001b[0;32m   1730\u001b[0m     steps_per_execution\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_steps_per_execution)\n",
      "\u001b[0;32m   1732\u001b[0m \u001b[39m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n",
      "\u001b[0;32m   1733\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(callbacks, callbacks_module\u001b[39m.\u001b[39mCallbackList):\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\keras\\engine\\data_adapter.py:1383\u001b[0m, in \u001b[0;36mget_data_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m   1381\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(kwargs[\u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39m_cluster_coordinator\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n",
      "\u001b[0;32m   1382\u001b[0m   \u001b[39mreturn\u001b[39;00m _ClusterCoordinatorDataHandler(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;32m-> 1383\u001b[0m \u001b[39mreturn\u001b[39;00m DataHandler(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\keras\\engine\\data_adapter.py:1138\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n",
      "\u001b[0;32m   1135\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_steps_per_execution_value \u001b[39m=\u001b[39m steps_per_execution\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mitem()\n",
      "\u001b[0;32m   1137\u001b[0m adapter_cls \u001b[39m=\u001b[39m select_data_adapter(x, y)\n",
      "\u001b[1;32m-> 1138\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_adapter \u001b[39m=\u001b[39m adapter_cls(\n",
      "\u001b[0;32m   1139\u001b[0m     x,\n",
      "\u001b[0;32m   1140\u001b[0m     y,\n",
      "\u001b[0;32m   1141\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n",
      "\u001b[0;32m   1142\u001b[0m     steps\u001b[39m=\u001b[39;49msteps_per_epoch,\n",
      "\u001b[0;32m   1143\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs \u001b[39m-\u001b[39;49m initial_epoch,\n",
      "\u001b[0;32m   1144\u001b[0m     sample_weights\u001b[39m=\u001b[39;49msample_weight,\n",
      "\u001b[0;32m   1145\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n",
      "\u001b[0;32m   1146\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n",
      "\u001b[0;32m   1147\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n",
      "\u001b[0;32m   1148\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n",
      "\u001b[0;32m   1149\u001b[0m     distribution_strategy\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mdistribute\u001b[39m.\u001b[39;49mget_strategy(),\n",
      "\u001b[0;32m   1150\u001b[0m     model\u001b[39m=\u001b[39;49mmodel)\n",
      "\u001b[0;32m   1152\u001b[0m strategy \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdistribute\u001b[39m.\u001b[39mget_strategy()\n",
      "\u001b[0;32m   1154\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_step \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\keras\\engine\\data_adapter.py:322\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n",
      "\u001b[0;32m    318\u001b[0m   \u001b[39mreturn\u001b[39;00m flat_dataset\n",
      "\u001b[0;32m    320\u001b[0m indices_dataset \u001b[39m=\u001b[39m indices_dataset\u001b[39m.\u001b[39mflat_map(slice_batch_indices)\n",
      "\u001b[1;32m--> 322\u001b[0m dataset \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mslice_inputs(indices_dataset, inputs)\n",
      "\u001b[0;32m    324\u001b[0m \u001b[39mif\u001b[39;00m shuffle \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbatch\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "\u001b[0;32m    325\u001b[0m   \u001b[39mdef\u001b[39;00m \u001b[39mshuffle_batch\u001b[39m(\u001b[39m*\u001b[39mbatch):\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\keras\\engine\\data_adapter.py:354\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.slice_inputs\u001b[1;34m(self, indices_dataset, inputs)\u001b[0m\n",
      "\u001b[0;32m    351\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgrab_batch\u001b[39m(i, data):\n",
      "\u001b[0;32m    352\u001b[0m   \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mmap_structure(\u001b[39mlambda\u001b[39;00m d: tf\u001b[39m.\u001b[39mgather(d, i, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m), data)\n",
      "\u001b[1;32m--> 354\u001b[0m dataset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39;49mmap(\n",
      "\u001b[0;32m    355\u001b[0m     grab_batch, num_parallel_calls\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mAUTOTUNE)\n",
      "\u001b[0;32m    357\u001b[0m \u001b[39m# Default optimizations are disabled to avoid the overhead of (unnecessary)\u001b[39;00m\n",
      "\u001b[0;32m    358\u001b[0m \u001b[39m# input pipeline graph serialization and deserialization\u001b[39;00m\n",
      "\u001b[0;32m    359\u001b[0m options \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mOptions()\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:1863\u001b[0m, in \u001b[0;36mDatasetV2.map\u001b[1;34m(self, map_func, num_parallel_calls, deterministic)\u001b[0m\n",
      "\u001b[0;32m   1861\u001b[0m   \u001b[39mreturn\u001b[39;00m MapDataset(\u001b[39mself\u001b[39m, map_func, preserve_cardinality\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;32m   1862\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;32m-> 1863\u001b[0m   \u001b[39mreturn\u001b[39;00m ParallelMapDataset(\n",
      "\u001b[0;32m   1864\u001b[0m       \u001b[39mself\u001b[39;49m,\n",
      "\u001b[0;32m   1865\u001b[0m       map_func,\n",
      "\u001b[0;32m   1866\u001b[0m       num_parallel_calls,\n",
      "\u001b[0;32m   1867\u001b[0m       deterministic,\n",
      "\u001b[0;32m   1868\u001b[0m       preserve_cardinality\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:5020\u001b[0m, in \u001b[0;36mParallelMapDataset.__init__\u001b[1;34m(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n",
      "\u001b[0;32m   5018\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input_dataset \u001b[39m=\u001b[39m input_dataset\n",
      "\u001b[0;32m   5019\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_use_inter_op_parallelism \u001b[39m=\u001b[39m use_inter_op_parallelism\n",
      "\u001b[1;32m-> 5020\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_map_func \u001b[39m=\u001b[39m StructuredFunctionWrapper(\n",
      "\u001b[0;32m   5021\u001b[0m     map_func,\n",
      "\u001b[0;32m   5022\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transformation_name(),\n",
      "\u001b[0;32m   5023\u001b[0m     dataset\u001b[39m=\u001b[39;49minput_dataset,\n",
      "\u001b[0;32m   5024\u001b[0m     use_legacy_function\u001b[39m=\u001b[39;49muse_legacy_function)\n",
      "\u001b[0;32m   5025\u001b[0m \u001b[39mif\u001b[39;00m deterministic \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;32m   5026\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_deterministic \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdefault\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:4218\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__\u001b[1;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n",
      "\u001b[0;32m   4211\u001b[0m       warnings\u001b[39m.\u001b[39mwarn(\n",
      "\u001b[0;32m   4212\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mEven though the `tf.config.experimental_run_functions_eagerly` \u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;32m   4213\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39moption is set, this option does not apply to tf.data functions. \u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;32m   4214\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mTo force eager execution of tf.data functions, please use \u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;32m   4215\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39m`tf.data.experimental.enable_debug_mode()`.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;32m   4216\u001b[0m     fn_factory \u001b[39m=\u001b[39m trace_tf_function(defun_kwargs)\n",
      "\u001b[1;32m-> 4218\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function \u001b[39m=\u001b[39m fn_factory()\n",
      "\u001b[0;32m   4219\u001b[0m \u001b[39m# There is no graph to add in eager mode.\u001b[39;00m\n",
      "\u001b[0;32m   4220\u001b[0m add_to_graph \u001b[39m&\u001b[39m\u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly()\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3150\u001b[0m, in \u001b[0;36mFunction.get_concrete_function\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m   3141\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_concrete_function\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n",
      "\u001b[0;32m   3142\u001b[0m   \u001b[39m\"\"\"Returns a `ConcreteFunction` specialized to inputs and execution context.\u001b[39;00m\n",
      "\u001b[0;32m   3143\u001b[0m \n",
      "\u001b[0;32m   3144\u001b[0m \u001b[39m  Args:\u001b[39;00m\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m   3148\u001b[0m \u001b[39m       or `tf.Tensor` or `tf.TensorSpec`.\u001b[39;00m\n",
      "\u001b[0;32m   3149\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n",
      "\u001b[1;32m-> 3150\u001b[0m   graph_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_concrete_function_garbage_collected(\n",
      "\u001b[0;32m   3151\u001b[0m       \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[0;32m   3152\u001b[0m   graph_function\u001b[39m.\u001b[39m_garbage_collector\u001b[39m.\u001b[39mrelease()  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "\u001b[0;32m   3153\u001b[0m   \u001b[39mreturn\u001b[39;00m graph_function\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3116\u001b[0m, in \u001b[0;36mFunction._get_concrete_function_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m   3114\u001b[0m   args, kwargs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;32m   3115\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n",
      "\u001b[1;32m-> 3116\u001b[0m   graph_function, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_define_function(args, kwargs)\n",
      "\u001b[0;32m   3117\u001b[0m   seen_names \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "\u001b[0;32m   3118\u001b[0m   captured \u001b[39m=\u001b[39m object_identity\u001b[39m.\u001b[39mObjectIdentitySet(\n",
      "\u001b[0;32m   3119\u001b[0m       graph_function\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39minternal_captures)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3463\u001b[0m, in \u001b[0;36mFunction._maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n",
      "\u001b[0;32m   3459\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_define_function_with_shape_relaxation(\n",
      "\u001b[0;32m   3460\u001b[0m       args, kwargs, flat_args, filtered_flat_args, cache_key_context)\n",
      "\u001b[0;32m   3462\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_cache\u001b[39m.\u001b[39mmissed\u001b[39m.\u001b[39madd(call_context_key)\n",
      "\u001b[1;32m-> 3463\u001b[0m graph_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_graph_function(args, kwargs)\n",
      "\u001b[0;32m   3464\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_cache\u001b[39m.\u001b[39mprimary[cache_key] \u001b[39m=\u001b[39m graph_function\n",
      "\u001b[0;32m   3466\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function, filtered_flat_args\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3298\u001b[0m, in \u001b[0;36mFunction._create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n",
      "\u001b[0;32m   3293\u001b[0m missing_arg_names \u001b[39m=\u001b[39m [\n",
      "\u001b[0;32m   3294\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (arg, i) \u001b[39mfor\u001b[39;00m i, arg \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(missing_arg_names)\n",
      "\u001b[0;32m   3295\u001b[0m ]\n",
      "\u001b[0;32m   3296\u001b[0m arg_names \u001b[39m=\u001b[39m base_arg_names \u001b[39m+\u001b[39m missing_arg_names\n",
      "\u001b[0;32m   3297\u001b[0m graph_function \u001b[39m=\u001b[39m ConcreteFunction(\n",
      "\u001b[1;32m-> 3298\u001b[0m     func_graph_module\u001b[39m.\u001b[39;49mfunc_graph_from_py_func(\n",
      "\u001b[0;32m   3299\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_name,\n",
      "\u001b[0;32m   3300\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_python_function,\n",
      "\u001b[0;32m   3301\u001b[0m         args,\n",
      "\u001b[0;32m   3302\u001b[0m         kwargs,\n",
      "\u001b[0;32m   3303\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_signature,\n",
      "\u001b[0;32m   3304\u001b[0m         autograph\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_autograph,\n",
      "\u001b[0;32m   3305\u001b[0m         autograph_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_autograph_options,\n",
      "\u001b[0;32m   3306\u001b[0m         arg_names\u001b[39m=\u001b[39;49marg_names,\n",
      "\u001b[0;32m   3307\u001b[0m         override_flat_arg_shapes\u001b[39m=\u001b[39;49moverride_flat_arg_shapes,\n",
      "\u001b[0;32m   3308\u001b[0m         capture_by_value\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_capture_by_value),\n",
      "\u001b[0;32m   3309\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_attributes,\n",
      "\u001b[0;32m   3310\u001b[0m     function_spec\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_spec,\n",
      "\u001b[0;32m   3311\u001b[0m     \u001b[39m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[39;00m\n",
      "\u001b[0;32m   3312\u001b[0m     \u001b[39m# scope. This is not the default behavior since it gets used in some\u001b[39;00m\n",
      "\u001b[0;32m   3313\u001b[0m     \u001b[39m# places (like Keras) where the FuncGraph lives longer than the\u001b[39;00m\n",
      "\u001b[0;32m   3314\u001b[0m     \u001b[39m# ConcreteFunction.\u001b[39;00m\n",
      "\u001b[0;32m   3315\u001b[0m     shared_func_graph\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;32m   3316\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:923\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n",
      "\u001b[0;32m    921\u001b[0m   arg_shapes \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;32m    922\u001b[0m   kwarg_shapes \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m--> 923\u001b[0m func_args \u001b[39m=\u001b[39m _get_defun_inputs_from_args(\n",
      "\u001b[0;32m    924\u001b[0m     args, arg_names, flat_shapes\u001b[39m=\u001b[39;49marg_shapes)\n",
      "\u001b[0;32m    925\u001b[0m func_kwargs \u001b[39m=\u001b[39m _get_defun_inputs_from_kwargs(\n",
      "\u001b[0;32m    926\u001b[0m     kwargs, flat_shapes\u001b[39m=\u001b[39mkwarg_shapes)\n",
      "\u001b[0;32m    928\u001b[0m \u001b[39m# Convert all Tensors into TensorSpecs before saving the structured inputs.\u001b[39;00m\n",
      "\u001b[0;32m    929\u001b[0m \u001b[39m# If storing pure concrete functions that are not called through polymorphic\u001b[39;00m\n",
      "\u001b[0;32m    930\u001b[0m \u001b[39m# functions, we don't have access to FunctionSpec, so we need to call the\u001b[39;00m\n",
      "\u001b[0;32m    931\u001b[0m \u001b[39m# TensorSpecs by their `arg_names` for later binding.\u001b[39;00m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1159\u001b[0m, in \u001b[0;36m_get_defun_inputs_from_args\u001b[1;34m(args, names, flat_shapes)\u001b[0m\n",
      "\u001b[0;32m   1157\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_defun_inputs_from_args\u001b[39m(args, names, flat_shapes\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n",
      "\u001b[0;32m   1158\u001b[0m   \u001b[39m\"\"\"Maps Python function positional args to graph-construction inputs.\"\"\"\u001b[39;00m\n",
      "\u001b[1;32m-> 1159\u001b[0m   \u001b[39mreturn\u001b[39;00m _get_defun_inputs(\n",
      "\u001b[0;32m   1160\u001b[0m       args, names, structure\u001b[39m=\u001b[39;49margs, flat_shapes\u001b[39m=\u001b[39;49mflat_shapes)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1232\u001b[0m, in \u001b[0;36m_get_defun_inputs\u001b[1;34m(args, names, structure, flat_shapes)\u001b[0m\n",
      "\u001b[0;32m   1230\u001b[0m placeholder_shape \u001b[39m=\u001b[39m shape \u001b[39mif\u001b[39;00m shape \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m arg\u001b[39m.\u001b[39mshape\n",
      "\u001b[0;32m   1231\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[1;32m-> 1232\u001b[0m   placeholder \u001b[39m=\u001b[39m graph_placeholder(\n",
      "\u001b[0;32m   1233\u001b[0m       arg\u001b[39m.\u001b[39;49mdtype, placeholder_shape,\n",
      "\u001b[0;32m   1234\u001b[0m       name\u001b[39m=\u001b[39;49mrequested_name)\n",
      "\u001b[0;32m   1235\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n",
      "\u001b[0;32m   1236\u001b[0m   \u001b[39m# Sometimes parameter names are not valid op names, so fall back to\u001b[39;00m\n",
      "\u001b[0;32m   1237\u001b[0m   \u001b[39m# unnamed placeholders.\u001b[39;00m\n",
      "\u001b[0;32m   1238\u001b[0m   placeholder \u001b[39m=\u001b[39m graph_placeholder(arg\u001b[39m.\u001b[39mdtype, placeholder_shape)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\eager\\graph_only_ops.py:38\u001b[0m, in \u001b[0;36mgraph_placeholder\u001b[1;34m(dtype, shape, name)\u001b[0m\n",
      "\u001b[0;32m     36\u001b[0m g \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mget_default_graph()\n",
      "\u001b[0;32m     37\u001b[0m attrs \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m: dtype_value, \u001b[39m\"\u001b[39m\u001b[39mshape\u001b[39m\u001b[39m\"\u001b[39m: shape}\n",
      "\u001b[1;32m---> 38\u001b[0m op \u001b[39m=\u001b[39m g\u001b[39m.\u001b[39;49m_create_op_internal(  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n",
      "\u001b[0;32m     39\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mPlaceholder\u001b[39;49m\u001b[39m\"\u001b[39;49m, [], [dtype], input_types\u001b[39m=\u001b[39;49m[],\n",
      "\u001b[0;32m     40\u001b[0m     attrs\u001b[39m=\u001b[39;49mattrs, name\u001b[39m=\u001b[39;49mname)\n",
      "\u001b[0;32m     41\u001b[0m result, \u001b[39m=\u001b[39m op\u001b[39m.\u001b[39moutputs\n",
      "\u001b[0;32m     42\u001b[0m \u001b[39mif\u001b[39;00m op_callbacks\u001b[39m.\u001b[39mshould_invoke_op_callbacks():\n",
      "\u001b[0;32m     43\u001b[0m   \u001b[39m# TODO(b/147670703): Once the special-op creation code paths\u001b[39;00m\n",
      "\u001b[0;32m     44\u001b[0m   \u001b[39m# are unified. Remove this `if` block.\u001b[39;00m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:599\u001b[0m, in \u001b[0;36mFuncGraph._create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n",
      "\u001b[0;32m    597\u001b[0m   inp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcapture(inp)\n",
      "\u001b[0;32m    598\u001b[0m   captured_inputs\u001b[39m.\u001b[39mappend(inp)\n",
      "\u001b[1;32m--> 599\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(FuncGraph, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m_create_op_internal(  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n",
      "\u001b[0;32m    600\u001b[0m     op_type, captured_inputs, dtypes, input_types, name, attrs, op_def,\n",
      "\u001b[0;32m    601\u001b[0m     compute_device)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:3561\u001b[0m, in \u001b[0;36mGraph._create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n",
      "\u001b[0;32m   3558\u001b[0m \u001b[39m# _create_op_helper mutates the new Operation. `_mutation_lock` ensures a\u001b[39;00m\n",
      "\u001b[0;32m   3559\u001b[0m \u001b[39m# Session.run call cannot occur between creating and mutating the op.\u001b[39;00m\n",
      "\u001b[0;32m   3560\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mutation_lock():\n",
      "\u001b[1;32m-> 3561\u001b[0m   ret \u001b[39m=\u001b[39m Operation(\n",
      "\u001b[0;32m   3562\u001b[0m       node_def,\n",
      "\u001b[0;32m   3563\u001b[0m       \u001b[39mself\u001b[39;49m,\n",
      "\u001b[0;32m   3564\u001b[0m       inputs\u001b[39m=\u001b[39;49minputs,\n",
      "\u001b[0;32m   3565\u001b[0m       output_types\u001b[39m=\u001b[39;49mdtypes,\n",
      "\u001b[0;32m   3566\u001b[0m       control_inputs\u001b[39m=\u001b[39;49mcontrol_inputs,\n",
      "\u001b[0;32m   3567\u001b[0m       input_types\u001b[39m=\u001b[39;49minput_types,\n",
      "\u001b[0;32m   3568\u001b[0m       original_op\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_default_original_op,\n",
      "\u001b[0;32m   3569\u001b[0m       op_def\u001b[39m=\u001b[39;49mop_def)\n",
      "\u001b[0;32m   3570\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_op_helper(ret, compute_device\u001b[39m=\u001b[39mcompute_device)\n",
      "\u001b[0;32m   3571\u001b[0m \u001b[39mreturn\u001b[39;00m ret\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:2041\u001b[0m, in \u001b[0;36mOperation.__init__\u001b[1;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n",
      "\u001b[0;32m   2039\u001b[0m   \u001b[39mif\u001b[39;00m op_def \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;32m   2040\u001b[0m     op_def \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_graph\u001b[39m.\u001b[39m_get_op_def(node_def\u001b[39m.\u001b[39mop)\n",
      "\u001b[1;32m-> 2041\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_c_op \u001b[39m=\u001b[39m _create_c_op(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_graph, node_def, inputs,\n",
      "\u001b[0;32m   2042\u001b[0m                             control_input_ops, op_def)\n",
      "\u001b[0;32m   2043\u001b[0m   name \u001b[39m=\u001b[39m compat\u001b[39m.\u001b[39mas_str(node_def\u001b[39m.\u001b[39mname)\n",
      "\u001b[0;32m   2045\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_traceback \u001b[39m=\u001b[39m tf_stack\u001b[39m.\u001b[39mextract_stack_for_node(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_c_op)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\ACER\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1880\u001b[0m, in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs, op_def)\u001b[0m\n",
      "\u001b[0;32m   1876\u001b[0m   pywrap_tf_session\u001b[39m.\u001b[39mTF_SetAttrValueProto(op_desc, compat\u001b[39m.\u001b[39mas_str(name),\n",
      "\u001b[0;32m   1877\u001b[0m                                          serialized)\n",
      "\u001b[0;32m   1879\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[1;32m-> 1880\u001b[0m   c_op \u001b[39m=\u001b[39m pywrap_tf_session\u001b[39m.\u001b[39;49mTF_FinishOperation(op_desc)\n",
      "\u001b[0;32m   1881\u001b[0m \u001b[39mexcept\u001b[39;00m errors\u001b[39m.\u001b[39mInvalidArgumentError \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;32m   1882\u001b[0m   \u001b[39m# Convert to ValueError for backwards compatibility.\u001b[39;00m\n",
      "\u001b[0;32m   1883\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mstr\u001b[39m(e))\n",
      "\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "enc_model = encoder_model() \n",
    "dec_model = decoder_model()\n",
    "\n",
    "# for _ in range(10):\n",
    "#     states_values1 , states_values2  = enc_model.predict( str_to_tokens( input( 'Enter question : ' ) ) )\n",
    "#     empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
    "#     \n",
    "#     empty_target_seq[0, 0] = tokenizer.word_index['start']\n",
    "#     \n",
    "#     stop_condition = False\n",
    "#     decoded_translation = ''\n",
    "#     while not stop_condition :\n",
    "#         print(\"alpha\")\n",
    "#         dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values1 + states_values2 )\n",
    "#         \n",
    "#         sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n",
    "#         sampled_word = None\n",
    "#         for word , index in tokenizer.word_index.items() :\n",
    "#             if sampled_word_index == index :\n",
    "#                 decoded_translation += ' {}'.format( word )\n",
    "#                 sampled_word = word\n",
    "        \n",
    "#         if sampled_word == 'end' or len(decoded_translation.split()) > maxlen_answers:\n",
    "#             stop_condition = True\n",
    "            \n",
    "#         empty_target_seq = np.zeros( ( 1 , 1 ) )  \n",
    "#         empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
    "#         states_values = [ h , c ] \n",
    "\n",
    "#     print( decoded_translation )\n",
    "\n",
    "for _ in range(10):\n",
    "    states_values1 , states_values2 = enc_model.predict( str_to_tokens( input( 'Enter question : ' ) ) )\n",
    "    empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
    "    empty_target_seq[0, 0] = tokenizer.word_index['start']\n",
    "    stop_condition = False\n",
    "    decoded_translation = ''\n",
    "    while not stop_condition :\n",
    "        # print(\"alpha\")\n",
    "        dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values1 + states_values2 )\n",
    "        # print(\"beta\")\n",
    "        sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n",
    "        sampled_word = None\n",
    "        # print(sampled_word_index)\n",
    "        for word , index in tokenizer.word_index.items() :\n",
    "            # print(word ,  index)\n",
    "            if sampled_word_index == index :\n",
    "                # print(\"here ----\")\n",
    "                decoded_translation += ' {}'.format( word )\n",
    "                sampled_word = word\n",
    "                print(decoded_translation)\n",
    "        # print(\"gamma\")\n",
    "        \n",
    "        if sampled_word == 'end' or len(decoded_translation.split()) > 5:\n",
    "            stop_condition = True\n",
    "            \n",
    "        empty_target_seq = np.zeros( ( 1 , 1 ) )  \n",
    "        empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
    "        states_values2 = [ h , c ] \n",
    "\n",
    "    print( decoded_translation )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('tfgpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "04ee3065d2333376d0aa8ad1893b6f4d1e1600d75ce606d1ee7a0507ce07e97f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
